{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multi-Layer Perceptron (MLP) - Large & Complex Data Set\n",
    "\n",
    "\n",
    "## Dataset\n",
    "In this notebook, we perform image recognition on the **MNIST dataset**, which contains a collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. Each image is labeled with the digit it represents. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixelâ€™s intensity, from 0 (white) to 255 (black). Hence each image in the set has 784 features.\n",
    "\n",
    "## Approach\n",
    "The goal is to identify the numbers using Pattern Recognition techniques. Image recognition is the ability AI to detect, classify and identify objects in images. Since the dataset contains hand-written digits (0-9), it is a multi-class classfication problem.\n",
    "\n",
    "We use **Multi-layer perception(MLP) classifier** for this purpose.\n",
    "\n",
    "To expedite the training time, we also use dimensionality reduction techniques (**Principle Component Analysis**) to project the features into a smaller dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Data Matrix (X) and the Label Vector (y)\n",
    "\n",
    "We load the data from a file or can load it from cloud using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Samples:  (70000, 784)\n",
      "No. of Labels:  (70000,)\n",
      "\n",
      "X type:  float64\n",
      "y type:  int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data.\n",
    "mnist = loadmat('mnist-original.mat')\n",
    "\n",
    "#Create the data Matrix X and the target vector y.\n",
    "X = mnist[\"data\"].T.astype('float64')\n",
    "y = mnist[\"label\"][0].astype('int64')\n",
    "\n",
    "\n",
    "#Load data using Scikit-Learn.\n",
    "# mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "# X = mnist[\"data\"].astype('float64')\n",
    "# y = mnist[\"target\"].astype('int64')\n",
    "\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y.shape)\n",
    "\n",
    "print(\"\\nX type: \", X.dtype)\n",
    "print(\"y type: \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets\n",
    "\n",
    "We spilt the dataset into training (80%) and test (20%) subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) #Set seed using random_state for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Using Dimensionaly Reduction\n",
    "\n",
    "Since every image in the data set has 785 features, we can expedite our model traning time by reducing the number of features. For this we use Principle Component Analysis(PCA) which is a dimension reduction technique.\n",
    "\n",
    "Since PCA is affected by the scale of the data, we also need to standardize the data before applying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Principle Components:  330\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=0.95) #We set n_components to 95%, since we want to retain 95% of the variance.\n",
    "\n",
    "pca.fit(X_train)\n",
    "\n",
    "print(\"Number of Principle Components: \", pca.n_components_) #Print the number of principle components after PCA.\n",
    "\n",
    "#Apply transformation to both the training and test set.\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: MLP \n",
    "\n",
    "First we train the MLP without applying PCA on the data. So we use all 784 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Use GridSearchCV for Hyperparameter Tuning to find parameters that give best performance.\n",
    "from sklearn.model_selection  import GridSearchCV\n",
    "\n",
    "param_grid = {'hidden_layer_sizes': [(100,), (150,), (200,)], \"solver\":['sgd', 'adam'], \n",
    "              'learning_rate_init': (0.1, 0.01, 0.001), \"alpha\": (0.1, 0.01),\n",
    "              'activation': ['logistic', 'relu']}\n",
    "\n",
    "clf_mlp = MLPClassifier(early_stopping=True, n_iter_no_change=10, tol=1e-5, max_iter=500, random_state=1)\n",
    "\n",
    "\n",
    "clf_mlp_cv = GridSearchCV(clf_mlp, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "clf_mlp_cv.fit(X_train_pca, y_train)\n",
    "\n",
    "params_optimal_mlp = clf_mlp_cv.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % clf_mlp_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_mlp)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49712015\n",
      "Validation score: 0.935357\n",
      "Iteration 2, loss = 0.19141485\n",
      "Validation score: 0.948750\n",
      "Iteration 3, loss = 0.14316349\n",
      "Validation score: 0.957679\n",
      "Iteration 4, loss = 0.11065116\n",
      "Validation score: 0.964286\n",
      "Iteration 5, loss = 0.08852195\n",
      "Validation score: 0.967679\n",
      "Iteration 6, loss = 0.07122619\n",
      "Validation score: 0.967857\n",
      "Iteration 7, loss = 0.06032630\n",
      "Validation score: 0.969821\n",
      "Iteration 8, loss = 0.05056308\n",
      "Validation score: 0.973214\n",
      "Iteration 9, loss = 0.04944693\n",
      "Validation score: 0.970893\n",
      "Iteration 10, loss = 0.04480906\n",
      "Validation score: 0.972500\n",
      "Iteration 11, loss = 0.03962610\n",
      "Validation score: 0.974286\n",
      "Iteration 12, loss = 0.04125116\n",
      "Validation score: 0.973750\n",
      "Iteration 13, loss = 0.03956745\n",
      "Validation score: 0.972500\n",
      "Iteration 14, loss = 0.03340966\n",
      "Validation score: 0.974464\n",
      "Iteration 15, loss = 0.02984075\n",
      "Validation score: 0.974107\n",
      "Iteration 16, loss = 0.03040872\n",
      "Validation score: 0.974286\n",
      "Iteration 17, loss = 0.03343423\n",
      "Validation score: 0.973929\n",
      "Iteration 18, loss = 0.03443790\n",
      "Validation score: 0.971607\n",
      "Iteration 19, loss = 0.03187373\n",
      "Validation score: 0.974821\n",
      "Iteration 20, loss = 0.03517368\n",
      "Validation score: 0.973393\n",
      "Iteration 21, loss = 0.02918387\n",
      "Validation score: 0.975000\n",
      "Iteration 22, loss = 0.02296873\n",
      "Validation score: 0.975536\n",
      "Iteration 23, loss = 0.02055751\n",
      "Validation score: 0.973750\n",
      "Iteration 24, loss = 0.02095178\n",
      "Validation score: 0.975893\n",
      "Iteration 25, loss = 0.02165685\n",
      "Validation score: 0.974286\n",
      "Iteration 26, loss = 0.02676042\n",
      "Validation score: 0.974821\n",
      "Iteration 27, loss = 0.02071113\n",
      "Validation score: 0.975357\n",
      "Iteration 28, loss = 0.02485249\n",
      "Validation score: 0.973929\n",
      "Iteration 29, loss = 0.02307029\n",
      "Validation score: 0.974643\n",
      "Iteration 30, loss = 0.02199665\n",
      "Validation score: 0.974464\n",
      "Iteration 31, loss = 0.01904912\n",
      "Validation score: 0.975893\n",
      "Iteration 32, loss = 0.01772241\n",
      "Validation score: 0.976786\n",
      "Iteration 33, loss = 0.01659847\n",
      "Validation score: 0.976429\n",
      "Iteration 34, loss = 0.01515413\n",
      "Validation score: 0.976786\n",
      "Iteration 35, loss = 0.01455228\n",
      "Validation score: 0.977143\n",
      "Iteration 36, loss = 0.01425937\n",
      "Validation score: 0.976429\n",
      "Iteration 37, loss = 0.01399844\n",
      "Validation score: 0.976071\n",
      "Iteration 38, loss = 0.01379790\n",
      "Validation score: 0.977321\n",
      "Iteration 39, loss = 0.01512395\n",
      "Validation score: 0.971786\n",
      "Iteration 40, loss = 0.05586040\n",
      "Validation score: 0.968214\n",
      "Iteration 41, loss = 0.04730927\n",
      "Validation score: 0.972321\n",
      "Iteration 42, loss = 0.02849427\n",
      "Validation score: 0.972500\n",
      "Iteration 43, loss = 0.02388159\n",
      "Validation score: 0.973571\n",
      "Iteration 44, loss = 0.02050189\n",
      "Validation score: 0.974821\n",
      "Iteration 45, loss = 0.01875617\n",
      "Validation score: 0.974464\n",
      "Iteration 46, loss = 0.01815569\n",
      "Validation score: 0.975179\n",
      "Iteration 47, loss = 0.01662963\n",
      "Validation score: 0.975536\n",
      "Iteration 48, loss = 0.01619659\n",
      "Validation score: 0.975893\n",
      "Iteration 49, loss = 0.01486778\n",
      "Validation score: 0.976429\n",
      "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The PCA+MLP takes 54.8s.\n",
      "No. of Iterations: 49\n",
      "\n",
      "Training Accuracy:  0.9976964285714286\n",
      "Wall time: 55.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "mlp_clf_pca = MLPClassifier(hidden_layer_sizes=(200,), max_iter=200, alpha=0.01,\n",
    "                    solver='adam', verbose=True, tol=1e-5, random_state=1, \n",
    "                    learning_rate='constant', learning_rate_init=0.001, activation='relu',\n",
    "                    early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "\n",
    "mlp_clf_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "duration_mlp_pca = t1 - t0\n",
    "print(\"The PCA+MLP takes {:.1f}s.\".format(duration_mlp_pca))\n",
    "\n",
    "print(\"No. of Iterations:\", mlp_clf_pca.n_iter_ )\n",
    "\n",
    "y_train_predicted = mlp_clf_pca.predict(X_train_pca)\n",
    "\n",
    "train_accuracy_mlp = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9754285714285714\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1300    0    1    1    0    1    4    3    2    0]\n",
      " [   0 1582   11    1    2    0    0    2    4    2]\n",
      " [   7    6 1317    4    2    1    2    4    3    2]\n",
      " [   1    1    9 1392    0   11    0    4    6    3]\n",
      " [   2    1    7    0 1325    1    8    2    2   14]\n",
      " [   2    1    3   12    1 1228   15    4   10    4]\n",
      " [   4    1    3    0    3    3 1381    1    1    0]\n",
      " [   2    0    8    3    8    1    1 1418    5   15]\n",
      " [   3    2    6    6    4    4    4    3 1354    4]\n",
      " [   5    2    3    6   20    4    0   12    8 1359]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1312\n",
      "           1       0.99      0.99      0.99      1604\n",
      "           2       0.96      0.98      0.97      1348\n",
      "           3       0.98      0.98      0.98      1427\n",
      "           4       0.97      0.97      0.97      1362\n",
      "           5       0.98      0.96      0.97      1280\n",
      "           6       0.98      0.99      0.98      1397\n",
      "           7       0.98      0.97      0.97      1461\n",
      "           8       0.97      0.97      0.97      1390\n",
      "           9       0.97      0.96      0.96      1419\n",
      "\n",
      "    accuracy                           0.98     14000\n",
      "   macro avg       0.98      0.98      0.98     14000\n",
      "weighted avg       0.98      0.98      0.98     14000\n",
      "\n",
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_predicted = mlp_clf_pca.predict(X_test_pca)\n",
    "\n",
    "accuracy_score_test_mlp_pca = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test_mlp_pca)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Classifier:  0.9754285714285714\n",
      "Running time:  54.84120059013367\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Classifier: \", accuracy_score_test_mlp_pca)\n",
    "print(\"Running time: \", duration_mlp_pca)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
